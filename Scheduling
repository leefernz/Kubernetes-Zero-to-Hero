So, in Kubernetes, by default, the scheduler automatically decides which node a pod should run on, based on available CPU, memory, and other factors.

But sometimes, we want control â€” like sending a pod to a specific type of node or avoiding certain nodes.
Thatâ€™s where these concepts â€” nodename, nodeSelector, node affinity, pod affinity, taints, and tolerations â€” come into play.

Let me go one by one ğŸ‘‡

ğŸ”¹ 1. nodeName

This is the most direct way to place a pod on a specific node.
You literally tell Kubernetes,

â€œRun this pod only on this node.â€

Example:

spec:
  nodeName: worker-node-1


Once you set this, the scheduler doesnâ€™t even make a decision â€” the pod just goes straight to that node.
Itâ€™s rarely used in production because itâ€™s very static â€” if that node goes down, the pod canâ€™t move elsewhere.

ğŸ”¹ 2. nodeSelector

A nodeSelector is a simple way to match pods with nodes based on labels.
So instead of hardcoding the node name, you add a label to nodes like:

kubectl label node worker-node-1 disktype=ssd


Then in your pod spec:

spec:
  nodeSelector:
    disktype: ssd


Now Kubernetes will place the pod only on nodes that have the label disktype=ssd.

Itâ€™s still simple and exact-match based, but better than using nodeName because itâ€™s label-based.

ğŸ”¹ 3. Node Affinity

Node affinity is a more flexible and advanced version of nodeSelector.
It lets you define soft and hard rules about where pods can or canâ€™t run.

requiredDuringSchedulingIgnoredDuringExecution: Hard rule â€” the pod must go on matching nodes.

preferredDuringSchedulingIgnoredDuringExecution: Soft rule â€” the pod should go there if possible.

Example:

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: disktype
          operator: In
          values:
          - ssd


So itâ€™s basically nodeSelector + flexibility + operator support (In, NotIn, Exists).
And we can even express preferences rather than strict rules.

ğŸ”¹ 4. Pod Affinity

Pod affinity is about scheduling pods close to other pods.
For example, if two services communicate frequently, you might want them on the same node or in the same zone to reduce latency.

Example:

affinity:
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchLabels:
          app: frontend
      topologyKey: "kubernetes.io/hostname"


This says â€” schedule my pod on the same node as the one running the frontend app.

ğŸ”¹ 5. Taints

Taints work in the opposite way â€” they repel pods from nodes.
Itâ€™s like marking a node with a condition that says,

â€œDonâ€™t put any pod here unless it can tolerate this condition.â€

Example:

kubectl taint nodes node1 key=value:NoSchedule


This means no pod will be scheduled on node1 unless it has a matching toleration.
Itâ€™s often used for dedicated or special-purpose nodes â€” like GPU nodes or infra nodes.

ğŸ”¹ 6. Tolerations

Tolerations are applied to pods, and they let those pods tolerate certain taints.
If a node is tainted, only pods with matching tolerations can land on it.

Example:

tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"


So taints repel, and tolerations allow exceptions.
Together, theyâ€™re used to control which workloads can or cannot run on certain nodes.
